{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Investment Risk_Prediction_Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQcRRCwLCOBA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "\n",
        "companies = pd.read_csv(\"companies.csv\")\n",
        "\n",
        "print(companies.shape)\n",
        "companies.head()\n",
        "\n",
        "classes = pd.value_counts(companies['status'])\n",
        "print(classes)\n",
        "size=17\n",
        "plt.figure(figsize=(7,5))\n",
        "ax=classes.plot(kind = 'bar', rot=0, fontsize=size, width=0.6)\n",
        "labels = ['Operating', 'Closed', 'Acquired', 'Ipo']\n",
        "plt.title(\"Distribution of Status\", fontsize=size)\n",
        "plt.xticks(range(len(labels)), labels)\n",
        "plt.xlabel(\"Status\", fontsize=size)\n",
        "plt.ylabel(\"Frequency\", fontsize=size)\n",
        "plt.grid(axis='y')\n",
        "x=0\n",
        "for i in ax.patches:\n",
        "    if x==0:\n",
        "      j=0\n",
        "    else:\n",
        "      j=0.1\n",
        "    ax.text(i.get_x()+j, i.get_height()+20, str(i.get_height()), fontsize=size)\n",
        "    x=x+1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"data.pdf\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "comp_filter['label'] = 0\n",
        "comp_filter.loc[comp_filter.status == 'ipo', 'label'] = 1\n",
        "comp_filter.loc[comp_filter.status == 'acquired', 'label'] = 1\n",
        "comp_filter.head()\n",
        "\n",
        "comp_filter.to_csv(\"comp_filter.csv\", index=False, encoding='utf8')\n",
        "df = pd.read_csv('comp_filter.csv')\n",
        "df.head()\n",
        "\n",
        "investor = pd.read_csv('investments.csv')\n",
        "investor_num = investor[['company_permalink',\n",
        "                         'investor_permalink']].groupby(['company_permalink']).agg(['count'])\n",
        "\n",
        "comp_plus_InvestorNum = pd.merge(how='inner',left=df, right=investor_num, \n",
        "                                 left_on='permalink', right_on='company_permalink')\n",
        "\n",
        "comp_plus_InvestorNum.to_csv('comp_plus_InvestorNum.csv',index=False, encoding='utf8')\n",
        "\n",
        "\n",
        "df = pd.read_csv('comp_plus_InvestorNum.csv')\n",
        "df.head()\n",
        "\n",
        "\n",
        "df.rename(columns={\"('investor_permalink', 'count')\": \"Num_of_investor\"}, inplace=True)\n",
        "df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')\n",
        "\n",
        "###funding_duration between 'first_funding_at' and 'last_funding_at'\n",
        "t1 = pd.to_datetime(df.first_funding_at, errors='coerce')\n",
        "t1 = pd.to_timedelta(t1).dt.days\n",
        "t2 = pd.to_datetime(df.last_funding_at, errors='coerce')\n",
        "t2 = pd.to_timedelta(t2).dt.days\n",
        "\n",
        "\n",
        "df['funding_duration'] = t2 - t1\n",
        "\n",
        "# convert datetime type \n",
        "df['first_funding_at_UTC'] = t1\n",
        "df['last_funding_at_UTC'] = t2\n",
        "\n",
        "df.info()\n",
        "\n",
        "df.to_csv('companies_allFeatures.csv', index=False, encoding='utf8')\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from scipy.sparse import hstack\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import resample, shuffle\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('companies_allFeatures.csv')\n",
        "\n",
        "\n",
        "#meaningful features\n",
        "df = df.drop(columns=['permalink', 'name', 'homepage_url', 'status', 'state_code',\n",
        "                      'region','city', 'founded_at', 'first_funding_at','last_funding_at'])\n",
        "\n",
        "df_clean = df.dropna()\n",
        "\n",
        "df.info()\n",
        "df_clean.info()\n",
        "\n",
        "df_clean.iloc[23]\n",
        "\n",
        "#split the dataset into tain, dev, and test set.\n",
        "X = df_clean.drop(columns=['label'])\n",
        "y = df_clean['label']\n",
        "X_train, X_test_, y_train, y_test_ = train_test_split(X, y, test_size=0.1, random_state=2,\n",
        "                                                      stratify=y, shuffle=True)\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_test_, y_test_, test_size=0.5, random_state=2,\n",
        "                                            shuffle=False)\n",
        "\n",
        "df = X_dev\n",
        "df['label'] = y_dev\n",
        "df.to_csv('dev.csv')\n",
        "\n",
        "df = X_test\n",
        "df['label'] = y_test\n",
        "df.to_csv('test.csv')\n",
        "\n",
        "df = X_train\n",
        "df['label'] = y_train\n",
        "df.to_csv('train.csv')\n",
        "\n",
        "df = pd.read_csv('train.csv', )\n",
        "df.shape\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df.label==0]\n",
        "print(df_majority.shape)\n",
        "df_minority = df[df.label==1]\n",
        "print(df_minority.shape)\n",
        "n = df.label.value_counts()[0]\n",
        "print(n)\n",
        "\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=n,    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "print(\"# after upsampled\")\n",
        "print(df_minority_upsampled.shape)\n",
        "\n",
        "# downsample majority class\n",
        "# df_majority_downsampled = resample(df_majority, \n",
        "#                                  replace=False,     # sample with replacement\n",
        "#                                  n_samples=n,    # to match majority class\n",
        "#                                  random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "print(\"after concat\")\n",
        "print(df_upsampled.shape)\n",
        "\n",
        "df_upsampled = shuffle(df_upsampled)\n",
        "\n",
        "\n",
        "n = df_upsampled.label.value_counts()[1]\n",
        "print(n)\n",
        "\n",
        "df_upsampled.to_csv('train_upsampled_shuffled.csv', index=False)\n",
        "\n",
        "df_upsampled.shape\n",
        "\n",
        "df = pd.read_csv('train_upsampled_shuffled.csv')\n",
        "X_train = df.iloc[:,1:-1]\n",
        "y_train = df.iloc[:,-1]\n",
        "\n",
        "\n",
        "df_dev = pd.read_csv('dev.csv')\n",
        "X_dev = df_dev.iloc[:,1:-1]\n",
        "y_dev = df_dev.iloc[:,-1]\n",
        "\n",
        "df_test = pd.read_csv('test.csv')\n",
        "X_test = df_test.iloc[:,1:-1]\n",
        "y_test = df_test.iloc[:,-1]\n",
        "\n",
        "df_dev.shape\n",
        "\n",
        "X_dev.shape\n",
        "\n",
        "X_train_text = X_train.category_list\n",
        "X_train_country = X_train.country_code\n",
        "X_train_nums = X_train.drop(columns=['category_list','country_code'])\n",
        "#################\n",
        "X_dev_text = X_dev.category_list\n",
        "X_dev_country = X_dev.country_code\n",
        "X_dev_nums = X_dev.drop(columns=['category_list','country_code'])\n",
        "##################\n",
        "X_test_text = X_test.category_list\n",
        "X_test_country = X_test.country_code\n",
        "X_test_nums = X_test.drop(columns=['category_list','country_code'])\n",
        "##################\n",
        "\n",
        "print(X_train_nums.columns.values)\n",
        "\n",
        "####text feature\n",
        "X_train.category_list = X_train.category_list.astype(str)\n",
        "vectorizer1 = CountVectorizer(min_df=5)\n",
        "vectorizer1.fit(X_train.category_list)\n",
        "\n",
        "X_train_text = vectorizer1.transform(X_train.category_list)\n",
        "X_dev_text = vectorizer1.transform(X_dev.category_list)\n",
        "X_test_text = vectorizer1.transform(X_test.category_list)\n",
        "\n",
        "\n",
        "#####categorical feature\n",
        "X_train.country_code= X_train.country_code.astype(str)\n",
        "vectorizer2 = CountVectorizer(min_df=1)\n",
        "vectorizer2.fit(X_train.category_list)\n",
        "X_train_country = vectorizer2.transform(X_train.country_code)\n",
        "X_dev_country = vectorizer2.transform(X_dev.country_code)\n",
        "X_test_country = vectorizer2.transform(X_test.country_code)\n",
        "\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "scaler.fit(X_train_nums)\n",
        "X_train_nums = scaler.transform(X_train_nums)\n",
        "X_dev_nums = scaler.transform(X_dev_nums)\n",
        "X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "\n",
        "print(X_train_nums.shape)\n",
        "print(X_train_country.shape)\n",
        "print(X_train_text.shape)\n",
        "#####concatinate inputs\n",
        "X_train_con = hstack([X_train_nums, X_train_country, X_train_text])\n",
        "X_dev_con = hstack([X_dev_nums, X_dev_country, X_dev_text])\n",
        "X_test_con = hstack([X_test_nums, X_test_country, X_test_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsgOZEdqf7mF"
      },
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "c=np.arange(1, 101, 10)\n",
        "gamma=np.array([1/108, 1/54, 1/27, 2/27, 4/27])\n",
        "depth=np.arange(991, 2991, 100)\n",
        "\n",
        "def train_svm(train_data, test_data, train_label, test_label, c_val, type, g):\n",
        "    if type=='gamma':\n",
        "        classifier = svm.SVC(C=c_val, kernel='rbf', gamma=g)\n",
        "    else:\n",
        "        classifier = svm.SVC(C=c_val, kernel='rbf', gamma='auto')\n",
        "    classifier.fit(train_data, train_label)\n",
        "    predicted_label = classifier.predict(test_data)\n",
        "    acc=accuracy_score(predicted_label, test_label)\n",
        "    return acc\n",
        "\n",
        "def plot_acc(avg_err, type):\n",
        "    global c\n",
        "    global gamma\n",
        "    if type=='c':\n",
        "        x=c\n",
        "    elif type=='gamma':\n",
        "        x=gamma\n",
        "    elif type==\"tree\":\n",
        "        x=depth\n",
        "    fig=plt.figure(1)\n",
        "    ax = fig.gca()\n",
        "    ax.set_xlim([min(x), max(x)])\n",
        "    if type=='c':\n",
        "        ax.set_xlabel('C')\n",
        "        ax.set_title('C vs Accuracy')\n",
        "        # plt.xscale('log')\n",
        "        # plt.grid(True, which=\"both\")\n",
        "    elif type=='gamma':\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xlabel('Gamma')\n",
        "        ax.set_title('Gamma vs Accuracy')\n",
        "    elif type==\"tree\":\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xlabel('Depth')\n",
        "        ax.set_title('Depth vs Accuracy')\n",
        "    plt.grid()\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    plt.plot(x, avg_err, 'r')\n",
        "    plt.show()\n",
        "\n",
        "def train_tree(train_data, test_data, train_label, test_label, depth):\n",
        "    classifier = DecisionTreeClassifier(max_depth=depth)\n",
        "    classifier.fit(train_data, train_label)\n",
        "    predicted_label = classifier.predict(test_data)\n",
        "    acc=accuracy_score(predicted_label, test_label)\n",
        "    return acc\n",
        "    \n",
        "def kfold(data, label, c_val, type, gamma, *args, **kwargs):\n",
        "    j=0\n",
        "    err=np.zeros((1, 10))\n",
        "    depth = kwargs.get('depth', None)\n",
        "    kfold = KFold(n_splits=10, shuffle=False)\n",
        "    for train_index, test_index in kfold.split(data):\n",
        "        print(\"k: \", j)\n",
        "        test_label=label[test_index]\n",
        "        test_data=data[test_index, :]\n",
        "        train_data=data[train_index, :]\n",
        "        train_label=label[train_index]\n",
        "        if type==\"tree\":\n",
        "          err[0, j]=train_tree(train_data, test_data, train_label, test_label, depth)\n",
        "        else:\n",
        "          err[0, j]=train_svm(train_data, test_data, train_label, test_label, c_val, type, gamma)\n",
        "        j=j+1\n",
        "    return err\n",
        "\n",
        "def evaluate(iter, data, label , c, type, err, *args, **kwargs):\n",
        "    gamma = kwargs.get('gamma', None)\n",
        "    d = kwargs.get('depth', None)\n",
        "    if iter==0:\n",
        "        err=kfold(data, label, c, type, gamma, depth=d)\n",
        "    else:\n",
        "        err=np.append(err, kfold(data, label, c, type, gamma), axis=0)\n",
        "    return err\n",
        "\n",
        "\n",
        "def evaluate_c(data, label, type):\n",
        "    err=np.array([])\n",
        "    global c\n",
        "    i=0\n",
        "    for c_val in c:\n",
        "        print(\"C: {0}\".format(c_val))\n",
        "        err=evaluate(i, data, label, c_val, type, err)\n",
        "        i=i+1\n",
        "    return err\n",
        "\n",
        "def evaluate_gamma(data, label, type, c):\n",
        "    global gamma\n",
        "    err=np.array([])\n",
        "    i=0\n",
        "    for g in gamma:\n",
        "        print(\"gamma: {0}, C: {1}\".format(g, c))\n",
        "        err=evaluate(i, data, label, c, type, err, gamma=g)\n",
        "        i=i+1\n",
        "    return err\n",
        "def evaluate_tree(data, label, type, c):\n",
        "    global depth\n",
        "    err=np.array([])\n",
        "    i=0\n",
        "    for d in depth:\n",
        "        print(\"depth: {0}\".format(d))\n",
        "        err=evaluate(i, data, label, c, type, err, depth=d)\n",
        "        i=i+1\n",
        "    return err\n",
        "\n",
        "    return err\n",
        "def train_model(data, label, type, *args, **kwargs):\n",
        "    # train=shuffle(train, random_state=0)\n",
        "    err=np.array([])\n",
        "    c = kwargs.get('best_c', None)\n",
        "    i=0\n",
        "    if type=='c':\n",
        "        err=evaluate_c(data, label, type)\n",
        "    elif type=='gamma':\n",
        "        err=evaluate_gamma(data, label, type, c)\n",
        "    elif type=='all':\n",
        "        err= evaluate(i, data, label, c, type, err)\n",
        "    elif type==\"tree\":\n",
        "        err=evaluate_tree(data, label, type, c)\n",
        "    avg_err=np.mean(err, axis=1)\n",
        "    if type=='all':\n",
        "        print(avg_err)\n",
        "    else:\n",
        "        plot_acc(avg_err, type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNJAgZsjomZ1"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, auc\n",
        "from scipy import interp\n",
        "from sklearn.metrics import f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "model_RF = DecisionTreeClassifier(max_depth=1391)\n",
        "model_RF.fit(X_train_con, y_train)\n",
        "print(X_dev_con.shape)\n",
        "y_pred = model_RF.predict(X_dev_con)\n",
        "acc = accuracy_score(y_dev, y_pred)\n",
        "\n",
        "f1 = f1_score(y_dev, y_pred)\n",
        "auc=roc_auc_score(y_dev, y_pred)\n",
        "print(acc)\n",
        "print(f1)\n",
        "print(auc)\n",
        "cm = confusion_matrix(y_dev, y_pred)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Oranges');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "plt.title('Decision Tree Classifier', size = 15);\n",
        "plt.show()\n",
        "\n",
        "\n",
        "tpr = cm[1,1]/(cm[1,1] + cm[1,0])\n",
        "fpr = cm[0,1]/(cm[0,1] + cm[0,0])\n",
        "recall=cm[1,1]/(cm[1,1] + cm[1,0])\n",
        "precision=cm[1,1]/(cm[1,1] + cm[0,1])\n",
        "print('TPR: {0}'.format(tpr))\n",
        "print('FPR: {0}'.format(fpr))\n",
        "print(\"recall:\", recall )\n",
        "print(\"precision:\", precision )\n",
        "print(\"f1: \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKF1XSOGadk9"
      },
      "source": [
        "print(X_dev_con.shape)\n",
        "y_pred = model_RF.predict(X_test_con)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc=roc_auc_score(y_test, y_pred)\n",
        "print(auc)\n",
        "print(acc)\n",
        "print(f1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Oranges');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "plt.title('Decision Tree Classifier', size = 15);\n",
        "plt.show()\n",
        "\n",
        "\n",
        "tpr = cm[1,1]/(cm[1,1] + cm[1,0])\n",
        "fpr = cm[0,1]/(cm[0,1] + cm[0,0])\n",
        "recall=cm[1,1]/(cm[1,1] + cm[1,0])\n",
        "precision=cm[1,1]/(cm[1,1] + cm[0,1])\n",
        "print('TPR: {0}'.format(tpr))\n",
        "print('FPR: {0}'.format(fpr))\n",
        "print(\"recall:\", recall )\n",
        "print(\"precision:\", precision )\n",
        "print(\"f1: \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zRqHELucXVG"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# X=X_train_con\n",
        "# y=y_train\n",
        "# forest = ExtraTreesClassifier(n_estimators=2500,\n",
        "#                               random_state=0)\n",
        "# forest.fit(X, y)\n",
        "# importances = forest.feature_importances_\n",
        "# std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "#              axis=0)\n",
        "# indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "# for f in range(X.shape[1]):\n",
        "#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "lst=[]\n",
        "stds=[]\n",
        "lst.append(sum(importances[6:811]))\n",
        "stds.append(np.sqrt(np.sum(np.square(std[6:811]))/2))\n",
        "lst.append(sum(importances[811:]))\n",
        "stds.append(np.sqrt(np.sum(np.square(std[811:]))/5))\n",
        "lst=np.array(lst+importances[0:6].tolist())\n",
        "stds=np.array(stds+std[0:6].tolist())\n",
        "indices2 = np.argsort(lst)[::-1]\n",
        "args=np.array(['Country\\nCode', 'Category\\nList', 'Funding\\nTotal\\n(USD)', 'Funding\\nRounds',\n",
        "        'Num of\\nInvestor', 'Funding\\nDuration', 'First Funding\\n at UTC', 'Last Funding\\n at UTC'])\n",
        "\n",
        "size=27\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.rcParams.update({'font.size': size})\n",
        "plt.title(\"Importance of Features\",fontsize=size)\n",
        "# plt.xlabel('Features')\n",
        "plt.ylabel('Percentage')\n",
        "plt.bar(range(lst.shape[0]), lst[indices2]*100,\n",
        "       color=\"r\", yerr=stds[indices2]*100, align=\"center\", capsize=10)\n",
        "plt.xticks(range(lst.shape[0]), args[indices2], rotation=-30, color='black')\n",
        "plt.yticks(color='black')\n",
        "plt.tight_layout()\n",
        "plt.savefig('demo.pdf', transparent=True)\n",
        "plt.show()\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "desJQENpl6jv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}